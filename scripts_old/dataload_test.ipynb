{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torcheeg\n",
      "  Downloading torcheeg-1.1.2.tar.gz (214 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.64.0 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from torcheeg) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.21.5 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from torcheeg) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.3.5 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from torcheeg) (1.5.3)\n",
      "Collecting xlrd>=2.0.1 (from torcheeg)\n",
      "  Downloading xlrd-2.0.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: scipy>=1.7.3 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from torcheeg) (1.14.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from torcheeg) (1.5.1)\n",
      "Collecting lmdb>=1.3.0 (from torcheeg)\n",
      "  Downloading lmdb-1.5.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: einops>=0.4.1 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from torcheeg) (0.8.0)\n",
      "Requirement already satisfied: mne>=1.0.3 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from torcheeg) (1.7.1)\n",
      "Collecting xmltodict>=0.13.0 (from torcheeg)\n",
      "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: networkx>=2.6.3 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from torcheeg) (3.3)\n",
      "Requirement already satisfied: PyWavelets>=1.3.0 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from torcheeg) (1.7.0)\n",
      "Collecting spectrum>=0.8.1 (from torcheeg)\n",
      "  Downloading spectrum-0.8.1.tar.gz (230 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torchmetrics>=0.10.0 (from torcheeg)\n",
      "  Downloading torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting mne_connectivity>=0.4.0 (from torcheeg)\n",
      "  Downloading mne_connectivity-0.7.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pytorch-lightning>=1.9.5 (from torcheeg)\n",
      "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: decorator in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from mne>=1.0.3->torcheeg) (5.1.1)\n",
      "Requirement already satisfied: jinja2 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from mne>=1.0.3->torcheeg) (3.1.4)\n",
      "Requirement already satisfied: lazy-loader>=0.3 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from mne>=1.0.3->torcheeg) (0.4)\n",
      "Requirement already satisfied: matplotlib>=3.5.0 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from mne>=1.0.3->torcheeg) (3.9.2)\n",
      "Requirement already satisfied: packaging in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from mne>=1.0.3->torcheeg) (24.1)\n",
      "Requirement already satisfied: pooch>=1.5 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from mne>=1.0.3->torcheeg) (1.8.2)\n",
      "Collecting netCDF4>=1.6.5 (from mne_connectivity>=0.4.0->torcheeg)\n",
      "  Downloading netCDF4-1.7.2-cp312-cp312-macosx_14_0_arm64.whl.metadata (1.8 kB)\n",
      "Collecting xarray>=2023.11.0 (from mne_connectivity>=0.4.0->torcheeg)\n",
      "  Downloading xarray-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from pandas>=1.3.5->torcheeg) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from pandas>=1.3.5->torcheeg) (2024.1)\n",
      "Requirement already satisfied: torch>=2.1.0 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from pytorch-lightning>=1.9.5->torcheeg) (2.4.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from pytorch-lightning>=1.9.5->torcheeg) (6.0.2)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=1.9.5->torcheeg) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from pytorch-lightning>=1.9.5->torcheeg) (4.12.2)\n",
      "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning>=1.9.5->torcheeg)\n",
      "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from scikit-learn>=1.0.2->torcheeg) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from scikit-learn>=1.0.2->torcheeg) (3.5.0)\n",
      "Collecting easydev (from spectrum>=0.8.1->torcheeg)\n",
      "  Downloading easydev-0.13.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>=2022.5.0->pytorch-lightning>=1.9.5->torcheeg)\n",
      "  Downloading aiohttp-3.11.7-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: setuptools in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from lightning-utilities>=0.10.0->pytorch-lightning>=1.9.5->torcheeg) (72.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from matplotlib>=3.5.0->mne>=1.0.3->torcheeg) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from matplotlib>=3.5.0->mne>=1.0.3->torcheeg) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from matplotlib>=3.5.0->mne>=1.0.3->torcheeg) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from matplotlib>=3.5.0->mne>=1.0.3->torcheeg) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from matplotlib>=3.5.0->mne>=1.0.3->torcheeg) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from matplotlib>=3.5.0->mne>=1.0.3->torcheeg) (3.1.2)\n",
      "Collecting cftime (from netCDF4>=1.6.5->mne_connectivity>=0.4.0->torcheeg)\n",
      "  Downloading cftime-1.6.4.post1-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: certifi in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from netCDF4>=1.6.5->mne_connectivity>=0.4.0->torcheeg) (2024.7.4)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from pooch>=1.5->mne>=1.0.3->torcheeg) (4.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from pooch>=1.5->mne>=1.0.3->torcheeg) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from python-dateutil>=2.8.1->pandas>=1.3.5->torcheeg) (1.16.0)\n",
      "Requirement already satisfied: filelock in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from torch>=2.1.0->pytorch-lightning>=1.9.5->torcheeg) (3.15.4)\n",
      "Requirement already satisfied: sympy in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from torch>=2.1.0->pytorch-lightning>=1.9.5->torcheeg) (1.13.2)\n",
      "Collecting pandas>=1.3.5 (from torcheeg)\n",
      "  Using cached pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from pandas>=1.3.5->torcheeg) (2024.1)\n",
      "Collecting colorama<0.5.0,>=0.4.6 (from easydev->spectrum>=0.8.1->torcheeg)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting colorlog<7.0.0,>=6.8.2 (from easydev->spectrum>=0.8.1->torcheeg)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting line-profiler<5.0.0,>=4.1.2 (from easydev->spectrum>=0.8.1->torcheeg)\n",
      "  Downloading line_profiler-4.1.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: pexpect<5.0.0,>=4.9.0 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from easydev->spectrum>=0.8.1->torcheeg) (4.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from jinja2->mne>=1.0.3->torcheeg) (2.1.5)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.9.5->torcheeg)\n",
      "  Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.9.5->torcheeg)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.9.5->torcheeg) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.9.5->torcheeg)\n",
      "  Using cached frozenlist-1.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.9.5->torcheeg)\n",
      "  Using cached multidict-6.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.9.5->torcheeg)\n",
      "  Using cached propcache-0.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.9.5->torcheeg)\n",
      "  Downloading yarl-1.18.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (67 kB)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from pexpect<5.0.0,>=4.9.0->easydev->spectrum>=0.8.1->torcheeg) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.0.3->torcheeg) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.0.3->torcheeg) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.0.3->torcheeg) (1.26.19)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages (from sympy->torch>=2.1.0->pytorch-lightning>=1.9.5->torcheeg) (1.3.0)\n",
      "Downloading lmdb-1.5.1-cp312-cp312-macosx_10_9_universal2.whl (166 kB)\n",
      "Downloading mne_connectivity-0.7.0-py3-none-any.whl (115 kB)\n",
      "Downloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.4/926.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
      "Downloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
      "Downloading netCDF4-1.7.2-cp312-cp312-macosx_14_0_arm64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xarray-2024.10.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl (11.4 MB)\n",
      "Downloading easydev-0.13.2-py3-none-any.whl (56 kB)\n",
      "Downloading aiohttp-3.11.7-cp312-cp312-macosx_11_0_arm64.whl (454 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading line_profiler-4.1.3-cp312-cp312-macosx_11_0_arm64.whl (132 kB)\n",
      "Downloading cftime-1.6.4.post1-cp312-cp312-macosx_11_0_arm64.whl (209 kB)\n",
      "Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached frozenlist-1.5.0-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\n",
      "Using cached multidict-6.1.0-cp312-cp312-macosx_11_0_arm64.whl (29 kB)\n",
      "Using cached propcache-0.2.0-cp312-cp312-macosx_11_0_arm64.whl (45 kB)\n",
      "Downloading yarl-1.18.0-cp312-cp312-macosx_11_0_arm64.whl (92 kB)\n",
      "Building wheels for collected packages: torcheeg, spectrum\n",
      "  Building wheel for torcheeg (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torcheeg: filename=torcheeg-1.1.2-py3-none-any.whl size=414909 sha256=df7d90a70d7765b3072c6dfeec86431c0c71713bbb9ee7ef2aacdfc08f669ada\n",
      "  Stored in directory: /Users/ldix/Library/Caches/pip/wheels/4e/28/dd/94a4011f0c70dae675b659fe4ddc63d60fb906cd5f5e7438af\n",
      "  Building wheel for spectrum (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for spectrum: filename=spectrum-0.8.1-cp312-cp312-macosx_11_0_arm64.whl size=226055 sha256=a67ec43a01d47bfaa29b59b156de2bd67fad393018ebc1a37261c0e87bc38bc5\n",
      "  Stored in directory: /Users/ldix/Library/Caches/pip/wheels/72/b1/5a/11624df545457cd6c467c55d8388027491c2ab143923213f0f\n",
      "Successfully built torcheeg spectrum\n",
      "Installing collected packages: lmdb, xmltodict, xlrd, propcache, multidict, line-profiler, lightning-utilities, frozenlist, colorlog, colorama, cftime, aiohappyeyeballs, yarl, pandas, netCDF4, easydev, aiosignal, xarray, torchmetrics, spectrum, aiohttp, mne_connectivity, pytorch-lightning, torcheeg\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.5.3\n",
      "    Uninstalling pandas-1.5.3:\n",
      "      Successfully uninstalled pandas-1.5.3\n",
      "Successfully installed aiohappyeyeballs-2.4.3 aiohttp-3.11.7 aiosignal-1.3.1 cftime-1.6.4.post1 colorama-0.4.6 colorlog-6.9.0 easydev-0.13.2 frozenlist-1.5.0 lightning-utilities-0.11.9 line-profiler-4.1.3 lmdb-1.5.1 mne_connectivity-0.7.0 multidict-6.1.0 netCDF4-1.7.2 pandas-2.2.3 propcache-0.2.0 pytorch-lightning-2.4.0 spectrum-0.8.1 torcheeg-1.1.2 torchmetrics-1.6.0 xarray-2024.10.0 xlrd-2.0.1 xmltodict-0.14.2 yarl-1.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torcheeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-11-21 18:28:28] INFO (torcheeg/MainThread) 🔍 | Processing EEG data. Processed EEG data has been cached to \u001b[92m.torcheeg/datasets_1732210108180_5Ac2E\u001b[0m.\n",
      "[2024-11-21 18:28:28] INFO (torcheeg/MainThread) ⏳ | Monitoring the detailed processing of a record for debugging. The processing of other records will only be reported in percentage to keep it clean.\n",
      "[PROCESS]: 0it [00:00, ?it/s]\n",
      "[2024-11-21 18:28:28] INFO (torcheeg/MainThread) ✅ | All processed EEG data has been cached to .torcheeg/datasets_1732210108180_5Ac2E.\n",
      "[2024-11-21 18:28:28] INFO (torcheeg/MainThread) 😊 | Please set \u001b[92mio_path\u001b[0m to \u001b[92m.torcheeg/datasets_1732210108180_5Ac2E\u001b[0m for the next run, to directly read from the cache if you wish to skip the data processing step.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 29\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# for i in range(num_files):\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#     n_samples = sfreq * duration\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#     data = np.random.randn(n_channels, n_samples)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#     file_path = folder / file_name\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#     raw.save(file_path)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m label_map \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfolder1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfolder2\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m}\n\u001b[0;32m---> 29\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mFolderDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstructure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msubject_in_label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mnum_channel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                        \u001b[49m\u001b[43monline_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mlabel_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLambda\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mnum_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bci/lib/python3.12/site-packages/torcheeg/datasets/module/folder_dataset.py:127\u001b[0m, in \u001b[0;36mFolderDataset.__init__\u001b[0;34m(self, root_path, structure, read_fn, online_transform, offline_transform, label_transform, io_path, io_size, io_mode, num_worker, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroot_path\u001b[39m\u001b[38;5;124m'\u001b[39m: root_path,\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstructure\u001b[39m\u001b[38;5;124m'\u001b[39m: structure,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m'\u001b[39m: verbose\n\u001b[1;32m    125\u001b[0m }\n\u001b[1;32m    126\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# save all arguments to __dict__\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(params)\n",
      "File \u001b[0;32m~/miniconda3/envs/bci/lib/python3.12/site-packages/torcheeg/datasets/module/base_dataset.py:122\u001b[0m, in \u001b[0;36mBaseDataset.__init__\u001b[0;34m(self, io_path, io_size, io_mode, num_worker, verbose, after_trial, after_session, after_subject, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     info_merged\u001b[38;5;241m.\u001b[39mappend(worker_info)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meeg_io_router \u001b[38;5;241m=\u001b[39m eeg_io_router\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo_merged\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter_trial \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter_session \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter_subject \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;66;03m# catch the exception\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/bci/lib/python3.12/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/miniconda3/envs/bci/lib/python3.12/site-packages/pandas/core/reshape/concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m~/miniconda3/envs/bci/lib/python3.12/site-packages/pandas/core/reshape/concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "from torcheeg.datasets import FolderDataset\n",
    "from torcheeg import transforms\n",
    "import mne\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "folder = Path('./root_folder')\n",
    "folder.mkdir(exist_ok=True)\n",
    "\n",
    "num_files = 10\n",
    "sfreq = 128  # Sampling rate\n",
    "n_channels = 14  # Number of channels\n",
    "duration = 5  # Data collected for 5 seconds\n",
    "# for i in range(num_files):\n",
    "#     n_samples = sfreq * duration\n",
    "#     data = np.random.randn(n_channels, n_samples)\n",
    "\n",
    "#     ch_names = [f'ch_{i+1:03}' for i in range(n_channels)]\n",
    "#     ch_types = ['eeg'] * n_channels\n",
    "#     info = mne.create_info(ch_names, sfreq, ch_types)\n",
    "#     raw = mne.io.RawArray(data, info)\n",
    "\n",
    "#     file_name = f'sub{i+1}.fif'\n",
    "#     file_path = folder / file_name\n",
    "#     raw.save(file_path)\n",
    "\n",
    "\n",
    "label_map = {'folder1': 0, 'folder2': 1}\n",
    "dataset = FolderDataset(root_path=folder,\n",
    "                        structure='subject_in_label',\n",
    "                        num_channel=14,\n",
    "                        online_transform=transforms.ToTensor(),\n",
    "                        label_transform=transforms.Compose([\n",
    "                            transforms.Select('label'),\n",
    "                            transforms.Lambda(lambda x: label_map[x])\n",
    "                        ]),\n",
    "                        num_worker=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 18:48:49,765 [ WARNING] Choosing from all possible events\n",
      "[2024-11-21 18:48:49] INFO (torcheeg/MainThread) 🔍 | Processing EEG data. Processed EEG data has been cached to \u001b[92m.torcheeg/io/moabb\u001b[0m.\n",
      "2024-11-21 18:48:49,766 [    INFO] 🔍 | Processing EEG data. Processed EEG data has been cached to \u001b[92m.torcheeg/io/moabb\u001b[0m.\n",
      "[2024-11-21 18:48:49] INFO (torcheeg/MainThread) ⏳ | Monitoring the detailed processing of a record for debugging. The processing of other records will only be reported in percentage to keep it clean.\n",
      "2024-11-21 18:48:49,767 [    INFO] ⏳ | Monitoring the detailed processing of a record for debugging. The processing of other records will only be reported in percentage to keep it clean.\n",
      "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/004-2014/B01T.mat' to file '/Users/ldix/Documents/Projects/moabb-generalised/.torcheeg/io/moabb/raw/MNE-bnci-data/database/data-sets/004-2014/B01T.mat'.\n",
      "/Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages/urllib3/connectionpool.py:1063: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|█████████████████████████████████████| 34.2M/34.2M [00:00<00:00, 38.3GB/s]\n",
      "SHA256 hash of downloaded file: 0da6e77ab0dab5b4aa1d2d5a6a542ac02f6768d3b7a76b0abe896ec1cf259919\n",
      "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
      "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/004-2014/B01E.mat' to file '/Users/ldix/Documents/Projects/moabb-generalised/.torcheeg/io/moabb/raw/MNE-bnci-data/database/data-sets/004-2014/B01E.mat'.\n",
      "/Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages/urllib3/connectionpool.py:1063: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|█████████████████████████████████████| 18.6M/18.6M [00:00<00:00, 25.1GB/s]\n",
      "SHA256 hash of downloaded file: 5effd365ae3733402286f2eea6b1ce482680a9f9ffc55c0b41bc63061a0161b5\n",
      "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
      "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/004-2014/B02T.mat' to file '/Users/ldix/Documents/Projects/moabb-generalised/.torcheeg/io/moabb/raw/MNE-bnci-data/database/data-sets/004-2014/B02T.mat'.\n",
      "/Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages/urllib3/connectionpool.py:1063: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|█████████████████████████████████████| 33.1M/33.1M [00:00<00:00, 45.2GB/s]\n",
      "SHA256 hash of downloaded file: 1c4ace3eee8d72ca184fa6995a9466939b71175b8b3a129bdaa838a85adf6473\n",
      "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
      "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/004-2014/B02E.mat' to file '/Users/ldix/Documents/Projects/moabb-generalised/.torcheeg/io/moabb/raw/MNE-bnci-data/database/data-sets/004-2014/B02E.mat'.\n",
      "/Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages/urllib3/connectionpool.py:1063: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|█████████████████████████████████████| 16.5M/16.5M [00:00<00:00, 19.2GB/s]\n",
      "SHA256 hash of downloaded file: f057ada16e36e7d58e5670b705ab41aec1175afde0a1e1e0b96cae58dbc7a083\n",
      "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
      "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/004-2014/B03T.mat' to file '/Users/ldix/Documents/Projects/moabb-generalised/.torcheeg/io/moabb/raw/MNE-bnci-data/database/data-sets/004-2014/B03T.mat'.\n",
      "/Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages/urllib3/connectionpool.py:1063: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|█████████████████████████████████████| 35.6M/35.6M [00:00<00:00, 23.9GB/s]\n",
      "SHA256 hash of downloaded file: 4ae1b23b4b4359151787a1a61d3acea128719677c49bfaf0113748cffece3b98\n",
      "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
      "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/004-2014/B03E.mat' to file '/Users/ldix/Documents/Projects/moabb-generalised/.torcheeg/io/moabb/raw/MNE-bnci-data/database/data-sets/004-2014/B03E.mat'.\n",
      "/Users/ldix/miniconda3/envs/bci/lib/python3.12/site-packages/urllib3/connectionpool.py:1063: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|█████████████████████████████████████| 19.3M/19.3M [00:00<00:00, 25.4GB/s]\n",
      "SHA256 hash of downloaded file: 6fca9dd9cbbdbb53dd8fb2798feed8e22669a91d1259fdac6695855756181353\n",
      "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
      "[PROCESS]:   0%|          | 0/15 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MotorImagery' object has no attribute 'process_raw'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m dataset\u001b[38;5;241m.\u001b[39msubject_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m      9\u001b[0m paradigm \u001b[38;5;241m=\u001b[39m MotorImagery()\n\u001b[0;32m---> 10\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mmoabb_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMOABBDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparadigm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparadigm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# io_path='./moabb',\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffline_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBandDifferentialEntropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43monline_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bci/lib/python3.12/site-packages/torcheeg/datasets/moabb/moabb_dataset.py:104\u001b[0m, in \u001b[0;36mMOABBDataset.__init__\u001b[0;34m(self, dataset, paradigm, chunk_size, overlap, online_transform, offline_transform, label_transform, before_trial, after_trial, after_session, after_subject, io_path, io_size, io_mode, download_path, num_worker, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not valid for paradigm\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(message)\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# save all arguments to __dict__\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(params)\n",
      "File \u001b[0;32m~/miniconda3/envs/bci/lib/python3.12/site-packages/torcheeg/datasets/module/base_dataset.py:71\u001b[0m, in \u001b[0;36mBaseDataset.__init__\u001b[0;34m(self, io_path, io_size, io_mode, num_worker, verbose, after_trial, after_session, after_subject, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;66;03m# shutil to delete the database\u001b[39;00m\n\u001b[1;32m     70\u001b[0m         shutil\u001b[38;5;241m.\u001b[39mrmtree(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mio_path)\n\u001b[0;32m---> 71\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# catch the exception\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/bci/lib/python3.12/site-packages/torcheeg/datasets/module/base_dataset.py:60\u001b[0m, in \u001b[0;36mBaseDataset.__init__\u001b[0;34m(self, io_path, io_size, io_mode, num_worker, verbose, after_trial, after_session, after_subject, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m     worker_results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file_id, file \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(records),\n\u001b[1;32m     54\u001b[0m                               disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m     55\u001b[0m                               desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[PROCESS]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     56\u001b[0m                               total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(records),\n\u001b[1;32m     57\u001b[0m                               position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     58\u001b[0m                               leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     59\u001b[0m         worker_results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m---> 60\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mio_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mio_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mfile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mprocess_record\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# shutil to delete the database\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mrmtree(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mio_path)\n",
      "File \u001b[0;32m~/miniconda3/envs/bci/lib/python3.12/site-packages/torcheeg/datasets/module/base_dataset.py:228\u001b[0m, in \u001b[0;36mBaseDataset.save_record\u001b[0;34m(io_path, io_size, io_mode, file, file_id, process_record, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;66;03m# call process_record of the class\u001b[39;00m\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;66;03m# get the current class name\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    231\u001b[0m             pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/bci/lib/python3.12/site-packages/torcheeg/datasets/moabb/moabb_dataset.py:129\u001b[0m, in \u001b[0;36mMOABBDataset.process_record\u001b[0;34m(file, chunk_size, overlap, offline_transform, dataset, paradigm, before_trial, after_trial, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m before_trial \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     run_signal \u001b[38;5;241m=\u001b[39m before_trial(run_signal)\n\u001b[0;32m--> 129\u001b[0m proc \u001b[38;5;241m=\u001b[39m \u001b[43mparadigm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_raw\u001b[49m(run_signal,\n\u001b[1;32m    130\u001b[0m                             dataset,\n\u001b[1;32m    131\u001b[0m                             return_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# this mean the run did not contain any selected event\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# go to next\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MotorImagery' object has no attribute 'process_raw'"
     ]
    }
   ],
   "source": [
    "import torcheeg.datasets.moabb as moabb_dataset\n",
    "from torcheeg import transforms\n",
    "\n",
    "from moabb.datasets import BNCI2014_004\n",
    "from moabb.paradigms import MotorImagery\n",
    "\n",
    "dataset = BNCI2014_004()\n",
    "dataset.subject_list = [1, 2, 3]\n",
    "paradigm = MotorImagery()\n",
    "dataset = moabb_dataset.MOABBDataset(\n",
    "    dataset=dataset,\n",
    "    paradigm=paradigm,\n",
    "    # io_path='./moabb',\n",
    "    offline_transform=transforms.Compose([transforms.BandDifferentialEntropy()\n",
    "                                          ]),\n",
    "    online_transform=transforms.ToTensor(),\n",
    "    label_transform=transforms.Compose([transforms.Select('label')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 128)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torcheeg.io.eeg_signal import EEGSignalIO\n",
    "import numpy as np\n",
    "\n",
    "eeg_io = EEGSignalIO('YOUR_PATH')\n",
    "key = eeg_io.write_eeg(np.random.randn(32, 128))\n",
    "eeg = eeg_io.read_eeg(key)\n",
    "eeg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '1',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = eeg_io.write_eeg((\"1\",np.random.randn(32, 128)))\n",
    "eeg_io.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1',\n",
       " array([[-3.38496695,  1.5466284 ,  1.39189547, ...,  0.22322036,\n",
       "          0.85847027,  1.10726473],\n",
       "        [-0.51665788, -1.40684739, -0.20040704, ...,  0.2280823 ,\n",
       "          0.13605188, -1.33143426],\n",
       "        [ 0.99644481,  0.89515923,  1.22433486, ...,  0.86915489,\n",
       "         -1.32050509, -0.7216613 ],\n",
       "        ...,\n",
       "        [ 0.48422514, -0.15618391,  0.65386516, ...,  0.17512069,\n",
       "         -0.20485026,  0.36536484],\n",
       "        [-0.02385757, -0.53372559, -0.75000119, ..., -0.14196311,\n",
       "         -1.27233509, -0.19526699],\n",
       "        [ 0.20142351,  0.46752788, -2.11397296, ..., -1.90734952,\n",
       "         -0.77076915,  0.30336441]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_io.read_eeg(\"16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make file to test speed\n",
    "\n",
    "filename = Path('./speed_results') / 'test.txt'\n",
    "filename.parent.mkdir(exist_ok=True)\n",
    "filename.touch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File_size = 532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making files: 100%|██████████| 10000/10000 [00:01<00:00, 9278.80it/s]\n",
      "Reading files:   0%|          | 0/313 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/ldix/miniconda3/envs/bci/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ldix/miniconda3/envs/bci/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'PickleDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Reading files:   0%|          | 0/313 [00:25<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 132\u001b[0m\n\u001b[1;32m    129\u001b[0m dataset \u001b[38;5;241m=\u001b[39m PickleDataset\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# debug_run(dataset)\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m \u001b[43mrun_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 113\u001b[0m, in \u001b[0;36mrun_test\u001b[0;34m(method, handler, settings)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sfreq \u001b[38;5;129;01min\u001b[39;00m settings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msfreq\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    112\u001b[0m     st \u001b[38;5;241m=\u001b[39m SpeedTester(n_subjects, n_trials, n_channels, trial_length, sfreq, handler)\n\u001b[0;32m--> 113\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_taken\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 81\u001b[0m, in \u001b[0;36mSpeedTester.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_data()\n\u001b[0;32m---> 81\u001b[0m time_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_taken\u001b[39m\u001b[38;5;124m'\u001b[39m: time_taken,\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_files\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_files,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msfreq\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msfreq,\n\u001b[1;32m     92\u001b[0m }\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "Cell \u001b[0;32mIn[11], line 71\u001b[0m, in \u001b[0;36mSpeedTester.read_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     70\u001b[0m     begin \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m---> 71\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mReading files\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mpass\u001b[39;49;00m\n\u001b[1;32m     73\u001b[0m     end \u001b[38;5;241m=\u001b[39m time()\n",
      "File \u001b[0;32m~/miniconda3/envs/bci/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bci/lib/python3.12/site-packages/torch/utils/data/dataloader.py:440\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bci/lib/python3.12/site-packages/torch/utils/data/dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bci/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1038\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1031\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1038\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m~/miniconda3/envs/bci/lib/python3.12/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bci/lib/python3.12/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bci/lib/python3.12/multiprocessing/context.py:289\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bci/lib/python3.12/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bci/lib/python3.12/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bci/lib/python3.12/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# setup dataset and dataloader from torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class PickleDataset(Dataset):\n",
    "    def __init__(self, metadata):\n",
    "        self.metadata = metadata\n",
    "        self.name = 'pickle'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filepath =  self.metadata.loc[idx, 'filepath']\n",
    "        y = self.metadata.loc[idx, 'label']\n",
    "        with open(filepath, 'rb') as f:\n",
    "            x = pkl.load(f)\n",
    "        return x, y\n",
    "    \n",
    "    def save_file(self, filepath, x):\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pkl.dump(x, f)\n",
    "\n",
    "class PickleDataset(Dataset):\n",
    "    def __init__(self, metadata):\n",
    "        self.metadata = metadata\n",
    "        self.name = 'pickle'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filepath =  self.metadata.loc[idx, 'filepath']\n",
    "        y = self.metadata.loc[idx, 'label']\n",
    "        with open(filepath, 'rb') as f:\n",
    "            x = pkl.load(f)\n",
    "        return x, y\n",
    "    \n",
    "    def save_file(self, filepath, x):\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pkl.dump(x, f)\n",
    "\n",
    "class SpeedTester:\n",
    "\n",
    "    def __init__(self, n_subjects, n_trials, n_channels, trial_length, sfreq, dataset):\n",
    "        self.n_subjects = n_subjects\n",
    "        self.n_trials = n_trials\n",
    "        self.n_channels = n_channels\n",
    "        self.trial_length = trial_length\n",
    "        self.sfreq = sfreq\n",
    "        self.num_files = n_subjects * n_trials\n",
    "        self.file_size = n_channels * int(trial_length * sfreq)\n",
    "        self.results_folder = Path('./speed_results')\n",
    "        self.results_folder.mkdir(exist_ok=True)\n",
    "        self.root_folder = Path('./speed_test')\n",
    "        self.root_folder.mkdir(exist_ok=True)\n",
    "        self.dataset = dataset\n",
    "        print(f\"\\nFile_size = {self.file_size}\")\n",
    "\n",
    "    def make_metadata(self):\n",
    "        metadata = []\n",
    "        for i in range(self.n_subjects):\n",
    "            for j in range(self.n_trials):\n",
    "                filepath = self.make_file_path(i, j)\n",
    "                metadata.append({'subject': i, 'trial': j, 'label': np.random.randint(1, 4), 'filepath': filepath})\n",
    "        self.metadata = pd.DataFrame(metadata)\n",
    "\n",
    "    def make_file_path(self, subj, trial):\n",
    "        return self.root_folder / f'sub-{subj}_{trial}.pkl'\n",
    "\n",
    "    def make_data(self):\n",
    "        # If data already exists, delete it\n",
    "        for file_path in self.root_folder.iterdir():\n",
    "            file_path.unlink()\n",
    "        for i in tqdm(range(len(self.metadata)), desc='Making files'):\n",
    "            file_path = self.metadata.loc[i, 'filepath']\n",
    "            x = np.random.randn(self.n_channels, int(self.trial_length * self.sfreq))\n",
    "            self.dataset.save_file(file_path, x)\n",
    "        \n",
    "\n",
    "    def read_data(self):\n",
    "        begin = time()\n",
    "        for x, y in tqdm(self.loader, desc='Reading files', total=len(self.loader)):\n",
    "            pass\n",
    "        end = time()\n",
    "        return end - begin\n",
    "\n",
    "    def run(self):\n",
    "        self.make_metadata()\n",
    "        self.dataset = self.dataset(self.metadata)\n",
    "        self.loader = DataLoader(self.dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "        self.make_data()\n",
    "        time_taken = self.read_data()\n",
    "\n",
    "        results = {\n",
    "            'time_taken': time_taken,\n",
    "            'num_files': self.num_files,\n",
    "            'file_size': self.file_size,\n",
    "            'n_subjects': self.n_subjects,\n",
    "            'n_trials': self.n_trials,\n",
    "            'n_channels': self.n_channels,\n",
    "            'trial_length': self.trial_length,\n",
    "            'sfreq': self.sfreq,\n",
    "        }\n",
    "\n",
    "        return results\n",
    "\n",
    "        \n",
    "\n",
    "def debug_run(dataset):\n",
    "    st = SpeedTester(10, 100, 10, 0.3, 64, dataset)\n",
    "    st.run()\n",
    "\n",
    "\n",
    "def run_test(method, handler, settings):\n",
    "    idx = 0\n",
    "    results_file = Path('./speed_results') / f'results_{method}.csv'\n",
    "    df = pd.DataFrame(columns=['time_taken', 'num_files', 'file_size', 'n_subjects', 'n_trials', 'n_channels', 'trial_length', 'sfreq'])\n",
    "    for n_subjects in settings['n_subjects']:\n",
    "        for n_trials in settings['n_trials']:\n",
    "            for n_channels in settings['n_channels']:\n",
    "                for trial_length in settings['trial_length']:\n",
    "                    for sfreq in settings['sfreq']:\n",
    "                        st = SpeedTester(n_subjects, n_trials, n_channels, trial_length, sfreq, handler)\n",
    "                        results = st.run()\n",
    "                        idx += 1\n",
    "                        print(f\"Run {idx}: {results['time_taken']:.2f} seconds\")\n",
    "                        df.loc[idx] = results\n",
    "                        df.to_csv(results_file, index=False)\n",
    " \n",
    "settings = {\n",
    "    'n_subjects' : [10],\n",
    "    'n_trials' : [1000],\n",
    "    'n_channels' : [14, 64],\n",
    "    'trial_length' : [0.3, 5],\n",
    "    'sfreq' : [128, 512]\n",
    "    # 'method' = ['pickle', 'hdf5']\n",
    "}\n",
    "\n",
    "method = 'PickleDataset_labeled_loader_4worker'\n",
    "dataset = PickleDataset\n",
    "\n",
    "# debug_run(dataset)\n",
    "run_test(method, dataset, settings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
